import gymnasium as gym
import numpy as np
import random
from collections import deque
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# ==================== é…ç½®matplotlibä¸­æ–‡å­—ä½“ ====================
# è§£å†³matplotlibä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# ==================== 1. è¶…å‚æ•°è®¾ç½® ====================
env_id = "CartPole-v1"
gamma = 0.99                    # æŠ˜æ‰£å› å­
learning_rate = 1e-3            # å­¦ä¹ ç‡
batch_size = 64                 # æ‰¹æ¬¡å¤§å°
buffer_capacity = 50_000        # ç»éªŒå›æ”¾ç¼“å†²åŒºå®¹é‡
start_training_after = 1_000    # å¼€å§‹è®­ç»ƒå‰éœ€è¦æ”¶é›†çš„ç»éªŒæ•°é‡
target_update_freq = 500         # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
epsilon_start = 1.0             # åˆå§‹æ¢ç´¢ç‡
epsilon_end = 0.05              # æœ€ç»ˆæ¢ç´¢ç‡
epsilon_decay = 5_000           # æ¢ç´¢ç‡è¡°å‡æ­¥æ•°
max_episodes = 500              # æœ€å¤§è®­ç»ƒè½®æ•°
max_steps = 500                 # æ¯è½®æœ€å¤§æ­¥æ•°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ==================== 2. åˆ›å»ºç¯å¢ƒ ====================
env = gym.make(env_id)
state_dim = env.observation_space.shape[0]  # çŠ¶æ€ç»´åº¦
action_dim = env.action_space.n             # åŠ¨ä½œç»´åº¦

print(f"çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")

# ==================== 3. ç»éªŒå›æ”¾ç¼“å†²åŒº ====================
class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œç”¨äºå­˜å‚¨å’Œé‡‡æ ·ç»éªŒ"""
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """å­˜å‚¨ä¸€æ¡ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))
        
        return (
            torch.tensor(states, dtype=torch.float32).to(device),
            torch.tensor(actions, dtype=torch.int64).unsqueeze(-1).to(device),
            torch.tensor(rewards, dtype=torch.float32).unsqueeze(-1).to(device),
            torch.tensor(next_states, dtype=torch.float32).to(device),
            torch.tensor(dones, dtype=torch.float32).unsqueeze(-1).to(device),
        )
    
    def __len__(self):
        return len(self.buffer)

memory = ReplayBuffer(buffer_capacity)

# ==================== 4. DQNç¥ç»ç½‘ç»œ ====================
class DQN(nn.Module):
    """æ·±åº¦Qç½‘ç»œ"""
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )
    
    def forward(self, x):
        return self.net(x)

# åˆ›å»ºç­–ç•¥ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
policy_net = DQN(state_dim, action_dim).to(device)
target_net = DQN(state_dim, action_dim).to(device)
# åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œï¼Œä½¿å…¶ä¸ç­–ç•¥ç½‘ç»œç›¸åŒ
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()  # ç›®æ ‡ç½‘ç»œè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# ä¼˜åŒ–å™¨
optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)
mse_loss = nn.MSELoss()

# ==================== 5. åŠ¨ä½œé€‰æ‹©å‡½æ•°ï¼ˆEpsilon-Greedyç­–ç•¥ï¼‰====================
def select_action(state, step):
    """
    ä½¿ç”¨epsilon-greedyç­–ç•¥é€‰æ‹©åŠ¨ä½œ
    è¿”å›: (åŠ¨ä½œ, å½“å‰epsilonå€¼)
    """
    # è®¡ç®—å½“å‰epsilonå€¼ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-step / epsilon_decay)
    
    if random.random() < epsilon:
        # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
        return env.action_space.sample(), epsilon
    else:
        # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = policy_net(state_tensor)
        return int(q_values.argmax().item()), epsilon

# ==================== 6. è®­ç»ƒå‡½æ•° ====================
def optimize_model():
    """ä»ç»éªŒå›æ”¾ç¼“å†²åŒºé‡‡æ ·å¹¶è®­ç»ƒç½‘ç»œ"""
    if len(memory) < batch_size:
        return
    
    # ä»ç¼“å†²åŒºé‡‡æ ·ä¸€æ‰¹ç»éªŒ
    states, actions, rewards, next_states, dones = memory.sample(batch_size)
    
    # è®¡ç®—å½“å‰Qå€¼
    q_values = policy_net(states).gather(1, actions)
    
    # è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆä½¿ç”¨ç›®æ ‡ç½‘ç»œï¼‰
    with torch.no_grad():
        # ç›®æ ‡Qå€¼ = å³æ—¶å¥–åŠ± + gamma * ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼ï¼ˆå¦‚æœæœªç»“æŸï¼‰
        max_next_q = target_net(next_states).max(1, keepdim=True)[0]
        target_q = rewards + gamma * (1 - dones) * max_next_q
    
    # è®¡ç®—æŸå¤±å¹¶æ›´æ–°ç½‘ç»œ
    loss = mse_loss(q_values, target_q)
    
    optimizer.zero_grad()
    loss.backward()
    # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)
    optimizer.step()

# ==================== 7. ä¸»è®­ç»ƒå¾ªç¯ ====================
print("\nğŸš€ å¼€å§‹è®­ç»ƒDQN...")
global_step = 0
all_rewards = []
all_epsilons = []

for episode in range(max_episodes):
    state, info = env.reset()
    episode_reward = 0
    
    for t in range(max_steps):
        # é€‰æ‹©åŠ¨ä½œ
        action, epsilon = select_action(state, global_step)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        # å­˜å‚¨ç»éªŒ
        memory.push(state, action, reward, next_state, done)
        
        # æ›´æ–°çŠ¶æ€
        state = next_state
        episode_reward += reward
        global_step += 1
        
        # å¦‚æœç¼“å†²åŒºæœ‰è¶³å¤Ÿç»éªŒï¼Œå¼€å§‹è®­ç»ƒ
        if global_step > start_training_after:
            optimize_model()
        
        # å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        if global_step % target_update_freq == 0:
            target_net.load_state_dict(policy_net.state_dict())
        
        if done:
            break
    
    all_rewards.append(episode_reward)
    all_epsilons.append(epsilon)
    
    # æ¯10è½®æ‰“å°ä¸€æ¬¡è¿›åº¦
    if (episode + 1) % 10 == 0:
        avg_reward = np.mean(all_rewards[-10:])
        print(f"Episode {episode+1:3d}/{max_episodes} | "
              f"å¹³å‡å¥–åŠ±: {avg_reward:6.1f} | "
              f"Epsilon: {epsilon:.3f} | "
              f"ç¼“å†²åŒºå¤§å°: {len(memory)}")

env.close()

# ==================== 8. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ ====================
print("\nğŸ“Š ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
plt.figure(figsize=(12, 5))

# å¥–åŠ±æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(all_rewards, alpha=0.3, color='blue', label='æ¯è½®å¥–åŠ±')
# è®¡ç®—ç§»åŠ¨å¹³å‡
window = 50
if len(all_rewards) >= window:
    moving_avg = np.convolve(all_rewards, np.ones(window)/window, mode='valid')
    plt.plot(np.arange(window-1, len(all_rewards)), moving_avg, 
             color='red', linewidth=2, label=f'{window}è½®ç§»åŠ¨å¹³å‡')
plt.xlabel('è½®æ•° (Episode)')
plt.ylabel('å¥–åŠ± (Reward)')
plt.title('DQNè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±å˜åŒ–')
plt.legend()
plt.grid(True)

# Epsilonè¡°å‡æ›²çº¿
plt.subplot(1, 2, 2)
plt.plot(all_epsilons, color='green')
plt.xlabel('è½®æ•° (Episode)')
plt.ylabel('Epsilon (æ¢ç´¢ç‡)')
plt.title('æ¢ç´¢ç‡è¡°å‡è¿‡ç¨‹')
plt.grid(True)

plt.tight_layout()
plt.savefig('dqn_training_curve.png', dpi=150)
print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º dqn_training_curve.png")
plt.show()

# ==================== è°ƒæ•´CartPoleæ¸²æŸ“çª—å£å¤§å°çš„å·¥å…·å‡½æ•° ====================
def resize_cartpole_window(environment, width=1200, height=800):
    """
    è°ƒæ•´CartPoleç¯å¢ƒçš„pygameçª—å£å¤§å°
    
    å‚æ•°:
        environment: gymnasiumç¯å¢ƒå¯¹è±¡
        width: çª—å£å®½åº¦ï¼ˆé»˜è®¤1200ï¼‰
        height: çª—å£é«˜åº¦ï¼ˆé»˜è®¤800ï¼‰
    """
    env_unwrapped = environment.unwrapped
    env_unwrapped.screen_width = width
    env_unwrapped.screen_height = height
    # å¦‚æœçª—å£å·²ç»åˆå§‹åŒ–ï¼Œéœ€è¦é‡æ–°åˆ›å»º
    if env_unwrapped.screen is not None:
        import pygame
        if env_unwrapped.render_mode == "human":
            env_unwrapped.screen = pygame.display.set_mode((width, height))
        else:
            env_unwrapped.screen = pygame.Surface((width, height))

# ==================== 9. è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ ====================
print("\nğŸ® å¼€å§‹è¯„ä¼°æ™ºèƒ½ä½“...")
eval_env = gym.make(env_id, render_mode="human")
# è°ƒæ•´çª—å£å¤§å°ï¼ˆé»˜è®¤1200x800ï¼Œå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ä¸ºå…¶ä»–å°ºå¯¸ï¼Œå¦‚1600x1000ï¼‰
resize_cartpole_window(eval_env, width=1200, height=800)
n_eval_episodes = 5
total_eval_rewards = 0

for episode in range(n_eval_episodes):
    state, info = eval_env.reset()
    # ç¡®ä¿çª—å£å¤§å°åœ¨resetåä»ç„¶æ­£ç¡®ï¼ˆå› ä¸ºresetå¯èƒ½ä¼šè§¦å‘æ¸²æŸ“ï¼‰
    resize_cartpole_window(eval_env, width=1200, height=800)
    done = False
    episode_reward = 0
    step_count = 0
    
    while not done and step_count < max_steps:
        # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥ç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼ˆä¸æ¢ç´¢ï¼‰
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = policy_net(state_tensor)
        action = int(q_values.argmax().item())
        
        state, reward, terminated, truncated, info = eval_env.step(action)
        done = terminated or truncated
        episode_reward += reward
        step_count += 1
        
        eval_env.render()
    
    total_eval_rewards += episode_reward
    print(f"è¯„ä¼°è½®æ¬¡ {episode+1}: å¥–åŠ± = {episode_reward}")

eval_env.close()

average_reward = total_eval_rewards / n_eval_episodes
print(f"\nâœ… è¯„ä¼°å®Œæˆï¼å¹³å‡å¥–åŠ±: {average_reward:.2f}")

print("\nğŸ‰ DQNè®­ç»ƒå’Œè¯„ä¼°å…¨éƒ¨å®Œæˆï¼")

