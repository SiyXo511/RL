
# Q-Learning 笔记

## 核心概念
- **目标**：学习一个 Q 函数 $Q(s,a)$，估计在状态 $s$ 采取动作 $a$ 的长期价值。
- **价值传播**：通过贝尔曼方程，将终点的奖励逐步“回传”到起点附近的状态-动作对。
- **策略**：通常使用 ε-greedy，平衡探索与利用。

## 数学基础：时序差分（TD）学习与贝尔曼方程

### 时序差分误差（TD Error）

在 Q-Learning 的更新公式中：

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \bigl( r + \gamma \max_{a'} Q(s', a') - Q(s,a) \bigr)
$$

其中，$r + \gamma \max_{a'} Q(s', a') - Q(s,a)$ 被称为**时序差分误差（Temporal Difference Error）**。

时序差分算法使用这个误差与步长（学习率）的乘积来更新状态值函数。

### 为什么可以用 $r_t + \gamma V(S_{t+1})$ 来代替 $G_t$？

在强化学习中，我们通常希望估计**总未来折扣回报** $G_t$：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

但在实际更新中，我们使用 $r_t + \gamma V(S_{t+1})$ 作为 $G_t$ 的估计。原因如下：

#### 贝尔曼方程的推导

对于策略 $\pi$ 下的状态值函数 $V_\pi(s)$，我们有：

$$
V_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
$$

将 $G_t$ 展开为未来折扣奖励的和：

$$
V_\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s\right]
$$

将立即奖励 $R_t$ 从求和式中分离出来：

$$
V_\pi(s) = \mathbb{E}_\pi\left[R_t + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s\right]
$$

注意到 $\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ 正是从下一状态 $S_{t+1}$ 开始的未来折扣回报，它等于 $V_\pi(S_{t+1})$。因此：

$$
V_\pi(s) = \mathbb{E}_\pi[R_t + \gamma V_\pi(S_{t+1}) | S_t = s]
$$

这就是**贝尔曼方程（Bellman Equation）**的核心形式。

#### 关键洞察

- **自举（Bootstrapping）**：我们使用对下一状态值 $V(S_{t+1})$ 的估计来更新当前状态值 $V(S_t)$，而不是等待完整的回报 $G_t$。
- **为什么有效**：因为 $V(S_{t+1})$ 本身已经包含了从 $S_{t+1}$ 开始的所有未来奖励的期望，所以 $R_t + \gamma V(S_{t+1})$ 是 $G_t$ 的一个合理估计。
- **在 Q-Learning 中的应用**：我们使用 $r + \gamma \max_{a'} Q(s', a')$ 作为目标值，这正是贝尔曼最优方程在 Q 函数上的体现。

## 算法流程
1. 初始化 Q 表（所有元素置 0）。

2. 对每个 episode 重复：
   - 从环境 `reset()` 得到初始状态。
   
   - 在未终止前循环：
     1. 依据 ε-greedy 选动作（ε 随训练逐步衰减）。
     
     2. 执行动作，得到 `(new_state, reward, done)`。
     
     3. 更新 Q 值：
        $$
        Q(s,a) \leftarrow Q(s,a) + \alpha \bigl( r + \gamma \max_{a'} Q(s', a') - Q(s,a) \bigr)
        $$
     
     4. 状态切换为 `new_state`。
     
   - episode 结束后衰减 ε，记录奖励。

## 重要超参数

### 基础参数

- **学习率 $\alpha$**：控制新经验对旧 Q 值的覆盖程度。
  - **小地图（4x4）**：可以使用较小的学习率（如 0.01），因为状态空间小，价值传播快。
  - **大地图（8x8）**：建议使用较大的学习率（如 0.1），因为：
    - 状态空间从 16 个增加到 64 个，需要更快的价值传播
    - 路径更长，奖励信号需要更远距离传播
    - 更大的学习率能让 Q 值更快收敛，避免训练时间过长

- **折扣因子 $\gamma$**：衡量未来奖励的重要性，通常接近 1（如 0.99）。
  - 对于更长的路径（如 8x8 地图），保持高折扣因子很重要，确保智能体能够看到长远的回报。

### 探索策略参数

- **ε 起始值 / 最小值**：
  - 初始 ε = 1.0，保证足够探索
  - 最小 ε ≈ 0.05（8x8 地图）或 0.01（4x4 地图），避免完全贪心

- **ε 衰减策略**：
  - **线性衰减**：在训练的前 80% 时间内逐步降低探索率
    - 优点：给智能体充足的探索时间，避免过早陷入局部最优
    - 实现：`epsilon_decay = (epsilon - min_epsilon) / (end_decay_episode - start_decay_episode)`
  - **指数衰减**：衰减速度更快，适合简单环境
  - **关键原则**：地图越大，探索期应该越长

### 训练规模参数

- **Episode 数**：
  - **4x4 地图**：通常 10,000-20,000 轮足够
  - **8x8 地图**：建议 50,000-100,000 轮
    - 原因：状态空间增加 4 倍，路径长度增加，需要更多样本才能充分探索和学习

- **每轮最大步数**：
  - **4x4 地图**：100-150 步通常足够
  - **8x8 地图**：建议 200 步或更多
    - 原因：从起点到终点的最短路径更长，需要更多步数才能完成一个 episode

## 常见问题与调参建议

### 训练效果差（奖励曲线为直线）

**症状**：训练过程中奖励曲线几乎是一条水平线，始终为 0，智能体无法学习到有效策略。

**原因分析**：
1. **探索不足**：智能体从未成功到达终点，Q 表中没有正奖励信号，价值无法传播
2. **训练轮数不足**：大地图需要更多轮次才能充分探索
3. **学习率过小**：价值传播太慢，即使偶尔到达终点，奖励也无法有效回传
4. **探索率衰减过快**：智能体过早停止探索，陷入局部最优（如一直向左撞墙）

**解决方案（针对 8x8 地图）**：

| 参数 | 问题值 | 推荐值 | 原因 |
|------|--------|--------|------|
| 学习率 | 0.01 | **0.1** | 加快价值传播，让奖励信号更快回传到起点 |
| Episode 数 | 10,000 | **60,000+** | 大地图需要更多探索，确保有足够机会到达终点 |
| ε 衰减策略 | 快速衰减 | **线性衰减，覆盖 80% 训练时间** | 给智能体充足探索期，避免过早贪心 |
| 每轮最大步数 | 150 | **200** | 8x8 地图路径更长，需要更多步数 |

**调参经验**：
- 如果奖励曲线是直线：**优先增加 episode 数和减慢 ε 衰减**
- 如果偶尔有奖励但增长很慢：**提高学习率**
- 如果训练后期奖励不再提升：**检查是否探索率降得太低，适当提高 min_epsilon**

### 其他常见问题

- **演示阶段卡在起点**：Q 表仍为 0 时 `argmax` 会返回索引 0（向左），可通过增大探索或添加 tie-breaking 解决。
- **窗口太小**：FrozenLake 可通过修改 `env.unwrapped.window_size` 放大渲染。

## 代码要点（`q_learning_frozen_lake.py`）

### 核心实现

- **初始化 Q 表**：`np.zeros((n_states, n_actions))`
- **更新公式**：对应代码第 96-98 行
  ```python
  q_table[state, action] = q_table[state, action] + learning_rate * (
      reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action]
  )
  ```

### 8x8 地图优化配置

```python
# 针对 8x8 地图的超参数设置
learning_rate = 0.1              # 提高学习率，加快价值传播
gamma = 0.99                     # 保持高折扣因子
epsilon = 1.0                    # 初始完全探索
min_epsilon = 0.05               # 最小探索率
n_episodes = 60000              # 大幅增加训练轮数
max_steps_per_episode = 200     # 增加每轮最大步数

# 线性衰减：在 80% 的训练时间内逐步降低探索率
start_decay_episode = 1
end_decay_episode = int(n_episodes * 0.8)
epsilon_decay = (epsilon - min_epsilon) / (end_decay_episode - start_decay_episode)
```

### 实用功能

- **策略可视化**：`print_policy()` 函数打印每个格子的最优动作方向（箭头表示）
- **线性衰减 ε**：使用 `start_epsilon_decay` 与 `end_epsilon_decay` 控制衰减区间
- **可视化**：
  - matplotlib 中文字体：`plt.rcParams['font.sans-serif'] = ['SimHei', ...]`
  - 奖励移动平均：`np.convolve(...)`
- **演示**：利用 `np.argmax(q_table[state,:])`，并配合扩大窗口的辅助函数 `resize_frozen_lake_window`

## 与 DQN 的区别
- Q-Learning 使用表格存储；DQN 用神经网络近似 Q 函数
- Q-Learning 适合状态空间小、离散；DQN 可以处理高维或连续状态
- DQN 引入经验回放和目标网络以稳定训练

## 记忆要点

1. **Q 表是“通关秘籍”**：通过不断试错填充，记录每个状态-动作对的价值。

2. **奖励反向传播**：奖励从终点开始，通过贝尔曼方程逐步“回传”到起点附近的状态-动作对。

3. **探索率是关键**：探索率的设计决定了智能体能否走出“撞墙”的局部最优。
   - 探索不足 → 奖励曲线为直线，无法学习
   - 探索过度 → 训练效率低，收敛慢
   - **解决方案**：线性衰减，在大部分训练时间内保持探索

4. **地图大小影响参数选择**：
   - **小地图（4x4）**：较小学习率（0.01）、较少 episode（10k-20k）
   - **大地图（8x8）**：较大学习率（0.1）、更多 episode（50k-100k）、更长探索期

5. **调参优先级**：
   - 奖励曲线为直线 → 优先增加 episode 数和减慢 ε 衰减
   - 偶尔有奖励但增长慢 → 提高学习率
   - 训练后期停滞 → 检查探索率是否过低

