import gymnasium as gym
import numpy as np
import random
import matplotlib.pyplot as plt
import time

# é…ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

# 1. åŠ è½½ç¯å¢ƒï¼ˆ8x8ï¼‰
env = gym.make("FrozenLake-v1", map_name="8x8", is_slippery=False)

# è°ƒæ•´FrozenLakeæ¸²æŸ“çª—å£å¤§å°çš„å·¥å…·å‡½æ•°
def resize_frozen_lake_window(environment, cell_pixels=128):
    """æ ¹æ®æŒ‡å®šå•å…ƒæ ¼åƒç´ å¤§å°ï¼Œæ”¾å¤§/ç¼©å°FrozenLakeçš„pygameçª—å£ã€‚"""
    env_unwrapped = environment.unwrapped
    window_width = cell_pixels * env_unwrapped.ncol
    window_height = cell_pixels * env_unwrapped.nrow
    env_unwrapped.window_size = (window_width, window_height)
    env_unwrapped.cell_size = (
        max(window_width // env_unwrapped.ncol, 1),
        max(window_height // env_unwrapped.nrow, 1),
    )

# 2. åˆå§‹åŒ–Qè¡¨
# è·å–çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„å¤§å°
n_states = env.observation_space.n
n_actions = env.action_space.n

# åˆå§‹åŒ–Qè¡¨ï¼Œæ‰€æœ‰å€¼éƒ½ä¸º0
q_table = np.zeros((n_states, n_actions))

# 3. è®¾ç½®è¶…å‚æ•°ï¼ˆé’ˆå¯¹ 8x8 åœ°å›¾åŠ å¤§è®­ç»ƒå¼ºåº¦ï¼‰
# å­¦ä¹ ç‡ï¼šé€‚å½“æé«˜ï¼Œè®©ä»·å€¼ä¼ æ’­æ›´å¿«
learning_rate = 0.1
# æŠ˜æ‰£å› å­ï¼šä¿æŒ 0.99ï¼Œé¼“åŠ±æ›´è¿œè§†é‡
gamma = 0.99

# Epsilon-greedy ç­–ç•¥è®¾ç½®
epsilon = 1.0          # åˆå§‹æ¢ç´¢ç‡
min_epsilon = 0.05     # æœ€å°æ¢ç´¢ç‡
n_episodes = 60000     # è®­ç»ƒè½®æ•°æ˜¾è‘—å¢åŠ 
max_steps_per_episode = 200

# ä½¿ç”¨çº¿æ€§è¡°å‡ï¼Œè®©æ™ºèƒ½ä½“åœ¨ 80% çš„è®­ç»ƒæ—¶é—´å†…é€æ­¥é™ä½æ¢ç´¢
start_decay_episode = 1
end_decay_episode = int(n_episodes * 0.8)
epsilon_decay = (epsilon - min_epsilon) / (end_decay_episode - start_decay_episode)

# ç”¨äºè®°å½•æ¯è½®çš„å¥–åŠ±
rewards_per_episode = []

def print_policy(table, environment):
    """æ‰“å°æ¯ä¸ªæ ¼å­çš„æœ€ä¼˜åŠ¨ä½œæ–¹å‘"""
    arrows = {0: "â†", 1: "â†“", 2: "â†’", 3: "â†‘"}
    desc = environment.unwrapped.desc
    nrow, ncol = environment.unwrapped.nrow, environment.unwrapped.ncol

    print("\nğŸ“Œ å½“å‰ç­–ç•¥ï¼ˆç®­å¤´ä»£è¡¨æœ€ä½³åŠ¨ä½œï¼‰:")
    for r in range(nrow):
        row_symbols = []
        for c in range(ncol):
            tile = desc[r, c].decode("utf-8")
            state_idx = r * ncol + c

            if tile in ("H", "G"):
                row_symbols.append(tile)
            else:
                best_action = int(np.argmax(table[state_idx, :]))
                row_symbols.append(arrows[best_action])
        print(" ".join(row_symbols))


# 4. Q-learningç®—æ³•
for episode in range(n_episodes):
    # é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹æ–°çš„ä¸€è½®
    state, info = env.reset()
    done = False
    episode_reward = 0

    for step in range(max_steps_per_episode):
        # Epsilon-greedyç­–ç•¥ï¼šé€‰æ‹©åŠ¨ä½œ
        if random.uniform(0, 1) < epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
            action = env.action_space.sample()
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
            action = np.argmax(q_table[state, :])

        # æ‰§è¡ŒåŠ¨ä½œï¼Œè§‚å¯Ÿæ–°çŠ¶æ€å’Œå¥–åŠ±
        new_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        # æ›´æ–°Qè¡¨
        # Q(s,a) = Q(s,a) + lr * [R(s,a) + gamma * max(Q(s',a')) - Q(s,a)]
        q_table[state, action] = q_table[state, action] + learning_rate * (
            reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action]
        )

        # æ›´æ–°çŠ¶æ€
        state = new_state
        # ç´¯åŠ å¥–åŠ±
        episode_reward += reward

        # å¦‚æœåˆ°è¾¾ç»ˆç‚¹ï¼Œåˆ™ç»“æŸæœ¬è½®
        if done:
            break

    # æ›´æ–°epsilonï¼ˆæ¢ç´¢ç‡è¡°å‡ï¼‰
    epsilon = max(min_epsilon, (epsilon - epsilon_decay))
    
    # è®°å½•æœ¬è½®å¥–åŠ±
    rewards_per_episode.append(episode_reward)

    # æ‰“å°è®­ç»ƒè¿›åº¦
    if (episode + 1) % 1000 == 0:
        print(f"Episode {episode + 1}/{n_episodes} - Epsilon: {epsilon:.4f}")

print("\nâœ… è®­ç»ƒå®Œæˆï¼")
print("\næœ€ç»ˆçš„Qè¡¨:")
print(q_table)
print_policy(q_table, env)

# 5. è¯„ä¼°æ™ºèƒ½ä½“çš„è¡¨ç°
print("\nğŸš€ å¼€å§‹è¯„ä¼°æ™ºèƒ½ä½“...")
n_eval_episodes = 100
total_eval_rewards = 0

for episode in range(n_eval_episodes):
    state, info = env.reset()
    done = False
    episode_reward = 0
    
    for step in range(max_steps_per_episode):
        # åœ¨è¯„ä¼°é˜¶æ®µï¼Œæˆ‘ä»¬åªåˆ©ç”¨å­¦åˆ°çš„ç­–ç•¥ï¼Œä¸è¿›è¡Œæ¢ç´¢
        action = np.argmax(q_table[state, :])
        new_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        state = new_state
        episode_reward += reward
        
        if done:
            break
            
    total_eval_rewards += episode_reward

average_reward = total_eval_rewards / n_eval_episodes
print(f"\nåœ¨ {n_eval_episodes} è½®è¯„ä¼°ä¸­çš„å¹³å‡å¥–åŠ±: {average_reward:.2f}")

# 6. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
plt.figure(figsize=(12, 6))
plt.plot(rewards_per_episode)
plt.xlabel("è½®æ•° (Episode)")
plt.ylabel("æ¯è½®çš„å¥–åŠ± (Reward)")
plt.title("Q-learning è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±å˜åŒ–")
# ä¸ºäº†æ›´å¥½åœ°å¯è§†åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶å¥–åŠ±çš„ç§»åŠ¨å¹³å‡çº¿
moving_avg_window = 100
moving_avg = np.convolve(rewards_per_episode, np.ones(moving_avg_window)/moving_avg_window, mode='valid')
plt.plot(np.arange(moving_avg_window - 1, len(rewards_per_episode)), moving_avg, color='red', linewidth=2, label=f'{moving_avg_window}è½®ç§»åŠ¨å¹³å‡å¥–åŠ±')
plt.legend()
plt.grid(True)
plt.show()

# 7. å¯è§†åŒ–æ™ºèƒ½ä½“å¦‚ä½•åˆ©ç”¨Qè¡¨æ¼”ç¤ºæœ€ä½³è·¯å¾„
print("\nğŸ§Š å±•ç¤ºæ™ºèƒ½ä½“ä»èµ·ç‚¹åˆ°ç»ˆç‚¹çš„æœ€ä½³è·¯å¾„...")
# åˆ›å»ºä¸€ä¸ªæ–°çš„ã€å¯æ¸²æŸ“çš„ç¯å¢ƒå®ä¾‹
# 'human'æ¨¡å¼ä¼šå¼¹å‡ºä¸€ä¸ªçª—å£æ¥æ˜¾ç¤ºåŠ¨ç”»
vis_env = gym.make("FrozenLake-v1", map_name="8x8", is_slippery=False, render_mode="human")
# å°†pygameçª—å£æ”¾å¤§ï¼Œé»˜è®¤æ¯ä¸ªæ ¼å­128åƒç´ ï¼ˆ4x4åœ°å›¾æ€»å®½é«˜512ï¼‰
resize_frozen_lake_window(vis_env, cell_pixels=80)
state, info = vis_env.reset()
done = False

# ç­‰å¾…ç”¨æˆ·æŒ‰é”®å¼€å§‹ï¼Œç¡®ä¿ç”¨æˆ·å‡†å¤‡å¥½è§‚çœ‹
print("å‡†å¤‡å¼€å§‹å¯è§†åŒ–ã€‚è¯·æŒ‰å›è½¦é”®å¯åŠ¨...")
print_policy(q_table, vis_env)
input()

for step in range(max_steps_per_episode):
    # æ¸²æŸ“å½“å‰å¸§
    vis_env.render()
    # æš‚åœä¸€ä¸‹ï¼Œæ–¹ä¾¿è‚‰çœ¼è§‚å¯Ÿ
    time.sleep(0.5)

    # ä»Qè¡¨ä¸­é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
    action = np.argmax(q_table[state, :])
    
    # æ‰§è¡ŒåŠ¨ä½œ
    new_state, reward, terminated, truncated, info = vis_env.step(action)
    done = terminated or truncated
    
    # æ›´æ–°çŠ¶æ€
    state = new_state
    
    if done:
        # æ¸²æŸ“æœ€åä¸€å¸§
        vis_env.render()
        if reward == 1.0:
            print("\nğŸ‰ æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼")
        else:
            print("\nâ˜ ï¸ ä¸å¹¸æ‰å…¥æ´ä¸­ã€‚")
        time.sleep(2) # åœ¨ç»“æŸå‰æš‚åœ2ç§’
        break

vis_env.close()


# 8. å…³é—­ç¯å¢ƒ
env.close()
