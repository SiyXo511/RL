import gymnasium as gym
import numpy as np
import random
import matplotlib.pyplot as plt
import time

# é…ç½®matplotlibä¸­æ–‡å­—ä½“ï¼Œé¿å…ä¸­æ–‡æ ‡ç­¾ä¹±ç 
plt.rcParams["font.sans-serif"] = ["SimHei", "Microsoft YaHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False

# 1. åˆ›å»º FrozenLake ç¯å¢ƒï¼ˆ8x8ï¼‰
env = gym.make("FrozenLake-v1", map_name="8x8", is_slippery=False)

# 2. åˆå§‹åŒ– Q è¡¨
n_states = env.observation_space.n
n_actions = env.action_space.n
q_table = np.zeros((n_states, n_actions))

# 3. å¤šæ­¥ SARSA è¶…å‚æ•°
learning_rate = 0.1
gamma = 0.99
n_step = 3  # å¤šæ­¥æ•°ï¼šä½¿ç”¨3æ­¥å›æŠ¥

epsilon = 1.0
min_epsilon = 0.05
n_episodes = 60000  # å¢åŠ è®­ç»ƒè½®æ•°
max_steps = 1000

# çº¿æ€§è¡°å‡ epsilon
start_decay_episode = 1
end_decay_episode = n_episodes // 2
epsilon_decay = (epsilon - min_epsilon) / (end_decay_episode - start_decay_episode)

rewards_history = []


def epsilon_greedy(state, eps):
    """Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
    if random.random() < eps:
        return env.action_space.sample()
    return int(np.argmax(q_table[state, :]))


def print_policy(table, environment):
    """æ‰“å°æ¯ä¸ªæ ¼å­çš„æœ€ä¼˜åŠ¨ä½œæ–¹å‘"""
    arrows = {0: "â†", 1: "â†“", 2: "â†’", 3: "â†‘"}
    desc = environment.unwrapped.desc
    nrow, ncol = environment.unwrapped.nrow, environment.unwrapped.ncol

    print("\nğŸ“Œ å½“å‰ç­–ç•¥ï¼ˆç®­å¤´ä»£è¡¨æœ€ä½³åŠ¨ä½œï¼‰:")
    for r in range(nrow):
        row_symbols = []
        for c in range(ncol):
            tile = desc[r, c].decode("utf-8")
            state_idx = r * ncol + c

            if tile in ("H", "G"):
                row_symbols.append(tile)
            else:
                best_action = int(np.argmax(table[state_idx, :]))
                row_symbols.append(arrows[best_action])
        print(" ".join(row_symbols))


# 4. å¤šæ­¥ SARSA è®­ç»ƒ
print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {n_step}-æ­¥ SARSA...")
print(f"å¤šæ­¥æ•°: {n_step}, å­¦ä¹ ç‡: {learning_rate}, æŠ˜æ‰£å› å­: {gamma}")

for episode in range(n_episodes):
    state, _ = env.reset()
    action = epsilon_greedy(state, epsilon)
    episode_reward = 0
    
    # å­˜å‚¨è½¨è¿¹ï¼šç”¨äºå¤šæ­¥æ›´æ–° [(state, action, reward), ...]
    trajectory = []
    
    for step in range(max_steps):
        # æ‰§è¡ŒåŠ¨ä½œ
        new_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        episode_reward += reward
        
        # å­˜å‚¨å½“å‰æ­¥åˆ°è½¨è¿¹
        trajectory.append((state, action, reward))
        
        # å¦‚æœè½¨è¿¹é•¿åº¦è¾¾åˆ° n_stepï¼Œè¿›è¡Œæ›´æ–°
        if len(trajectory) >= n_step:
            # è·å–è¦æ›´æ–°çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼ˆæœ€è€çš„ï¼‰
            s_t, a_t, _ = trajectory[0]
            
            # è®¡ç®—næ­¥å›æŠ¥
            # R_t^(n) = r_{t+1} + Î³*r_{t+2} + ... + Î³^{n-1}*r_{t+n} + Î³^n * Q(s_{t+n}, a_{t+n})
            n_step_return = 0
            for i in range(n_step):
                n_step_return += (gamma ** i) * trajectory[i][2]  # ç´¯ç§¯å¥–åŠ±
            
            # å¦‚æœæœªç»“æŸï¼ŒåŠ ä¸Šnæ­¥åçš„Qå€¼
            if not done:
                next_action = epsilon_greedy(new_state, epsilon)
                n_step_return += (gamma ** n_step) * q_table[new_state, next_action]
                # æ›´æ–°çŠ¶æ€å’ŒåŠ¨ä½œä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨
                state = new_state
                action = next_action
            else:
                # Episodeç»“æŸï¼Œåªä½¿ç”¨å®é™…å¥–åŠ±
                state = new_state
                action = None
            
            # æ›´æ–°Qå€¼
            td_error = n_step_return - q_table[s_t, a_t]
            q_table[s_t, a_t] += learning_rate * td_error
            
            # ç§»é™¤æœ€è€çš„å…ƒç´ ï¼Œä¿æŒè½¨è¿¹é•¿åº¦ä¸ºn_step-1ï¼ˆä¸‹æ¬¡å¾ªç¯ä¼šæ·»åŠ æ–°çš„ï¼‰
            trajectory.pop(0)
        
        else:
            # è½¨è¿¹è¿˜ä¸å¤Ÿé•¿ï¼Œç»§ç»­æ”¶é›†
            state = new_state
            if not done:
                action = epsilon_greedy(new_state, epsilon)
            else:
                action = None
        
        if done:
            # Episodeç»“æŸï¼Œæ›´æ–°å‰©ä½™çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼ˆä½¿ç”¨å®é™…æ­¥æ•°ï¼‰
            if len(trajectory) > 0:
                for idx in range(len(trajectory)):
                    s_t, a_t, _ = trajectory[idx]
                    # è®¡ç®—ä»idxåˆ°ç»“æŸçš„å›æŠ¥
                    remaining_return = 0
                    for i in range(idx, len(trajectory)):
                        remaining_return += (gamma ** (i - idx)) * trajectory[i][2]
                    # æ›´æ–°Qå€¼
                    td_error = remaining_return - q_table[s_t, a_t]
                    q_table[s_t, a_t] += learning_rate * td_error
            break
    
    rewards_history.append(episode_reward)
    
    # æ›´æ–°epsilon
    if start_decay_episode <= episode <= end_decay_episode:
        epsilon = max(min_epsilon, epsilon - epsilon_decay)
    
    # æ‰“å°è¿›åº¦
    if (episode + 1) % 2000 == 0:
        avg_reward = np.mean(rewards_history[-1000:])
        print(
            f"Episode {episode + 1}/{n_episodes} - epsilon: {epsilon:.3f} - avg_reward(last 1000): {avg_reward:.3f}"
        )

print("\nâœ… å¤šæ­¥ SARSA è®­ç»ƒå®Œæˆï¼")
print("æœ€ç»ˆ Q è¡¨ï¼š")
print_policy(q_table, env)

# 5. è¯„ä¼°
print("\nğŸš€ å¼€å§‹è¯„ä¼°æ™ºèƒ½ä½“...")
eval_episodes = 100
success = 0

for _ in range(eval_episodes):
    state, _ = env.reset()
    for _ in range(max_steps):
        action = int(np.argmax(q_table[state, :]))
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        if done:
            success += reward
            break

print(f"\nè¯„ä¼°æˆåŠŸç‡ï¼š{success / eval_episodes:.2%}")

# 6. å¯è§†åŒ–è®­ç»ƒå¥–åŠ±
plt.figure(figsize=(12, 5))
plt.plot(rewards_history, alpha=0.4, label="æ¯è½®å¥–åŠ±", color='blue')

window = 200
if len(rewards_history) >= window:
    moving_avg = np.convolve(
        rewards_history, np.ones(window) / window, mode="valid"
    )
    plt.plot(
        np.arange(window - 1, len(rewards_history)),
        moving_avg,
        color="red",
        linewidth=2,
        label=f"{window} è½®ç§»åŠ¨å¹³å‡",
    )

plt.xlabel("Episode")
plt.ylabel("Reward")
plt.title(f"{n_step}-æ­¥ SARSA è®­ç»ƒå¥–åŠ±æ›²çº¿")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig(f'n_step_sarsa_training_curve.png', dpi=150)
print(f"\nè®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º n_step_sarsa_training_curve.png")
plt.show()

# 7. pygame å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
print("\nğŸ§Š å±•ç¤ºæ™ºèƒ½ä½“ä»èµ·ç‚¹åˆ°ç»ˆç‚¹çš„æœ€ä½³è·¯å¾„...")
vis_env = gym.make("FrozenLake-v1", map_name="8x8", is_slippery=False, render_mode="human")

# è°ƒæ•´çª—å£å¤§å°
def resize_frozen_lake_window(environment, cell_pixels=80):
    """æ ¹æ®æŒ‡å®šå•å…ƒæ ¼åƒç´ å¤§å°ï¼Œæ”¾å¤§/ç¼©å°FrozenLakeçš„pygameçª—å£ã€‚"""
    env_unwrapped = environment.unwrapped
    window_width = cell_pixels * env_unwrapped.ncol
    window_height = cell_pixels * env_unwrapped.nrow
    env_unwrapped.window_size = (window_width, window_height)
    env_unwrapped.cell_size = (
        max(window_width // env_unwrapped.ncol, 1),
        max(window_height // env_unwrapped.nrow, 1),
    )

resize_frozen_lake_window(vis_env, cell_pixels=80)
state, _ = vis_env.reset()
print_policy(q_table, vis_env)
input("\næŒ‰å›è½¦é”®å¼€å§‹æ¼”ç¤º...")

for _ in range(max_steps):
    vis_env.render()
    time.sleep(0.5)
    action = int(np.argmax(q_table[state, :]))
    state, reward, terminated, truncated, _ = vis_env.step(action)
    done = terminated or truncated
    if done:
        vis_env.render()
        if reward == 1.0:
            print("ğŸ‰ æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼")
        else:
            print("ğŸ˜¢ ä¸æ…æ‰å…¥æ´ä¸­ã€‚")
        break

vis_env.close()
env.close()

