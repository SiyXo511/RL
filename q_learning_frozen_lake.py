import gymnasium as gym
import numpy as np
import random
import matplotlib.pyplot as plt
import time

# 1. åŠ è½½ç¯å¢ƒ
env = gym.make("FrozenLake-v1", is_slippery=False)

# 2. åˆå§‹åŒ–Qè¡¨
# è·å–çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„å¤§å°
n_states = env.observation_space.n
n_actions = env.action_space.n

# åˆå§‹åŒ–Qè¡¨ï¼Œæ‰€æœ‰å€¼éƒ½ä¸º0
q_table = np.zeros((n_states, n_actions))

# 3. è®¾ç½®è¶…å‚æ•°
# å­¦ä¹ ç‡ï¼šå†³å®šäº†æˆ‘ä»¬å¤šå¤§ç¨‹åº¦ä¸Šæ¥å—æ–°çš„Qå€¼
learning_rate = 0.9
# æŠ˜æ‰£å› å­ï¼šè¡¡é‡æœªæ¥å¥–åŠ±çš„é‡è¦æ€§
gamma = 0.9
# Epsilon-greedyç­–ç•¥ä¸­çš„epsilon
epsilon = 1.0       # åˆå§‹æ¢ç´¢ç‡
epsilon_decay = 0.0001 # æ¢ç´¢ç‡è¡°å‡å€¼
min_epsilon = 0.01   # æœ€å°æ¢ç´¢ç‡

# è®­ç»ƒçš„æ€»è½®æ•°
n_episodes = 10000
# æ¯è½®çš„æœ€å¤§æ­¥æ•°
max_steps_per_episode = 100

# ç”¨äºè®°å½•æ¯è½®çš„å¥–åŠ±
rewards_per_episode = []

# 4. Q-learningç®—æ³•
for episode in range(n_episodes):
    # é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹æ–°çš„ä¸€è½®
    state, info = env.reset()
    done = False
    episode_reward = 0

    for step in range(max_steps_per_episode):
        # Epsilon-greedyç­–ç•¥ï¼šé€‰æ‹©åŠ¨ä½œ
        if random.uniform(0, 1) < epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
            action = env.action_space.sample()
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
            action = np.argmax(q_table[state, :])

        # æ‰§è¡ŒåŠ¨ä½œï¼Œè§‚å¯Ÿæ–°çŠ¶æ€å’Œå¥–åŠ±
        new_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        # æ›´æ–°Qè¡¨
        # Q(s,a) = Q(s,a) + lr * [R(s,a) + gamma * max(Q(s',a')) - Q(s,a)]
        q_table[state, action] = q_table[state, action] + learning_rate * (
            reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action]
        )

        # æ›´æ–°çŠ¶æ€
        state = new_state
        # ç´¯åŠ å¥–åŠ±
        episode_reward += reward

        # å¦‚æœåˆ°è¾¾ç»ˆç‚¹ï¼Œåˆ™ç»“æŸæœ¬è½®
        if done:
            break

    # æ›´æ–°epsilonï¼ˆæ¢ç´¢ç‡è¡°å‡ï¼‰
    epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay))
    
    # è®°å½•æœ¬è½®å¥–åŠ±
    rewards_per_episode.append(episode_reward)

    # æ‰“å°è®­ç»ƒè¿›åº¦
    if (episode + 1) % 1000 == 0:
        print(f"Episode {episode + 1}/{n_episodes} - Epsilon: {epsilon:.4f}")

print("\nâœ… è®­ç»ƒå®Œæˆï¼")
print("\næœ€ç»ˆçš„Qè¡¨:")
print(q_table)

# 5. è¯„ä¼°æ™ºèƒ½ä½“çš„è¡¨ç°
print("\nğŸš€ å¼€å§‹è¯„ä¼°æ™ºèƒ½ä½“...")
n_eval_episodes = 100
total_eval_rewards = 0

for episode in range(n_eval_episodes):
    state, info = env.reset()
    done = False
    episode_reward = 0
    
    for step in range(max_steps_per_episode):
        # åœ¨è¯„ä¼°é˜¶æ®µï¼Œæˆ‘ä»¬åªåˆ©ç”¨å­¦åˆ°çš„ç­–ç•¥ï¼Œä¸è¿›è¡Œæ¢ç´¢
        action = np.argmax(q_table[state, :])
        new_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        state = new_state
        episode_reward += reward
        
        if done:
            break
            
    total_eval_rewards += episode_reward

average_reward = total_eval_rewards / n_eval_episodes
print(f"\nåœ¨ {n_eval_episodes} è½®è¯„ä¼°ä¸­çš„å¹³å‡å¥–åŠ±: {average_reward:.2f}")

# 6. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
plt.figure(figsize=(12, 6))
plt.plot(rewards_per_episode)
plt.xlabel("è½®æ•° (Episode)")
plt.ylabel("æ¯è½®çš„å¥–åŠ± (Reward)")
plt.title("Q-learning è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±å˜åŒ–")
# ä¸ºäº†æ›´å¥½åœ°å¯è§†åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶å¥–åŠ±çš„ç§»åŠ¨å¹³å‡çº¿
moving_avg_window = 100
moving_avg = np.convolve(rewards_per_episode, np.ones(moving_avg_window)/moving_avg_window, mode='valid')
plt.plot(np.arange(moving_avg_window - 1, len(rewards_per_episode)), moving_avg, color='red', linewidth=2, label=f'{moving_avg_window}è½®ç§»åŠ¨å¹³å‡å¥–åŠ±')
plt.legend()
plt.grid(True)
plt.show()

# 7. å¯è§†åŒ–æ™ºèƒ½ä½“å¦‚ä½•åˆ©ç”¨Qè¡¨æ¼”ç¤ºæœ€ä½³è·¯å¾„
print("\nğŸ§Š å±•ç¤ºæ™ºèƒ½ä½“ä»èµ·ç‚¹åˆ°ç»ˆç‚¹çš„æœ€ä½³è·¯å¾„...")
# åˆ›å»ºä¸€ä¸ªæ–°çš„ã€å¯æ¸²æŸ“çš„ç¯å¢ƒå®ä¾‹
# 'human'æ¨¡å¼ä¼šå¼¹å‡ºä¸€ä¸ªçª—å£æ¥æ˜¾ç¤ºåŠ¨ç”»
vis_env = gym.make("FrozenLake-v1", is_slippery=False, render_mode="human")
state, info = vis_env.reset()
done = False

# ç­‰å¾…ç”¨æˆ·æŒ‰é”®å¼€å§‹ï¼Œç¡®ä¿ç”¨æˆ·å‡†å¤‡å¥½è§‚çœ‹
print("å‡†å¤‡å¼€å§‹å¯è§†åŒ–ã€‚è¯·æŒ‰å›è½¦é”®å¯åŠ¨...")
input()

for step in range(max_steps_per_episode):
    # æ¸²æŸ“å½“å‰å¸§
    vis_env.render()
    # æš‚åœä¸€ä¸‹ï¼Œæ–¹ä¾¿è‚‰çœ¼è§‚å¯Ÿ
    time.sleep(0.5)

    # ä»Qè¡¨ä¸­é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
    action = np.argmax(q_table[state, :])
    
    # æ‰§è¡ŒåŠ¨ä½œ
    new_state, reward, terminated, truncated, info = vis_env.step(action)
    done = terminated or truncated
    
    # æ›´æ–°çŠ¶æ€
    state = new_state
    
    if done:
        # æ¸²æŸ“æœ€åä¸€å¸§
        vis_env.render()
        if reward == 1.0:
            print("\nğŸ‰ æˆåŠŸåˆ°è¾¾ç»ˆç‚¹ï¼")
        else:
            print("\nâ˜ ï¸ ä¸å¹¸æ‰å…¥æ´ä¸­ã€‚")
        time.sleep(2) # åœ¨ç»“æŸå‰æš‚åœ2ç§’
        break

vis_env.close()


# 8. å…³é—­ç¯å¢ƒ
env.close()
